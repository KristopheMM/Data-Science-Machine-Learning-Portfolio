# Resources

### Difference between Random Forests and Decision tree
As is implied by the names "Tree" and "Forest," a Random Forest is essentially a collection of Decision Trees. A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results. After a large number of trees are built using this method, each tree "votes" or chooses the class, and the class receiving the most votes by a simple majority is the "winner" or predicted class. There are of course some more detailed differences, but this is the main conceptual difference.

- [Theory on explanation of Decision Tree and Random Forest](https://medium.com/datadriveninvestor/decision-tree-and-random-forest-e174686dd9eb)
- [Random Forest Regression model explained in depth(Theory)](https://medium.com/@george.drakos62/random-forest-regression-model-explained-in-depth-f2cce437c750)
- [A Friendly Introduction to Random Forest(Theory)](https://medium.com/capital-one-tech/random-forest-algorithm-for-machine-learning-c4b2c8cc9feb)
- [Decision Trees: Understanding the Basis of Ensemble Methods](https://towardsdatascience.com/decision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704)
- [Complete guide to Decision Trees(By Marco Peixeiro)](https://towardsdatascience.com/the-complete-guide-to-decision-trees-17a874301448)
- [A practical End-to-End Random Forest algorithm example](https://towardsdatascience.com/random-forest-in-python-24d0893d51c0)
